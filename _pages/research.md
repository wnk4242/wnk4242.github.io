---
layout: archive
title: "Research"
permalink: /research/
author_profile: true
---
## Research Replicability
Over the last decade, psychology has faced a replication crisis because efforts to replicate past study findings often fail to yield consistent results. Psychologists have conducted several large-scale multi-lab replication projects (such as the Many Labs series) to reassess the credibility of earlier findings. I am currently pursuing three lines of research to enhance our understanding of research replicability:

- The application of Bayesian meta-analysis in the evaluation of direct replications

   Psychologists are currently faced with a pressing methodological challenge: developing a  framework to evaluate outcomes of replications. In my dissertation, I propose adopting the Bayesian 
   hypothesis testing within a meta-analytic framework and explore how well such an approach answers the following question: _“Do the pooled data from all replications support or refute the presence of the psychological 
   effect?”_ More importantly, my research expands our understanding by answering an essential follow-up question: _“How often are our conclusions about the replicability of original studies incorrect, based on their 
   replications?”_ My long-term objective is to develop a empirically validated statistical framework for the assessment of replications of advanced and complex research designs. 

![Shiny App](/images/beta-regression-model.png)

- Credibility of research findings in educational psychology
  
   Statistical power plays a key role in ensuring the reliability, precision, and replicability of effect size estimation. In psychology, the average power is distressingly low—estimated at 36% with only 8% of studies deemed 
   adequately powered. The impact of such issues on educational psychology has not received the attention it deserves. I am currently researching the credibility of meta-analyses published in five 
   leading educational psychology journals between 2012 and 2022. Completion of this work will raise awareness within the academic community about the prevalence of under-powered research in top-tier journals. A working paper (AERA proposal) is [available here.](https://wnk4242.github.io/files/power paper.pdf) Peer-reviewers' feedback is [available here.](https://wnk4242.github.io/files/power paper feedback.pdf)

- Effect size, heterogeneity, and power of direct replications

   Effect-size variability (also known as heterogeneity), is a key methodological factor that is frequently neglected in planning the sample sizes for replication studies. My research investigates several fundamental 
   questions regarding heterogeneity in replication studies and examines how the potentially linear relationship between effect sizes and heterogeneity may impact the statistical power of replications. Gaining a better 
   understanding of heterogeneity is key to improving sample size planning for future multi-lab replication projects. An AERA iPoster is [available here.](https://aera24-aera.ipostersessions.com/?s=8C-A3-3D-B1-3C-88-5E-CB-83-7D-3D-0C-C3-0A-CA-A7)

## Meta-analysis of Proportions

An important application of meta-analytic methods is pooling single proportions to estimate the overall prevalence of a specific condition. In 2017 alone, 152 meta-analyses of proportions were published across various scientific disciplines, such as medicine, public health, epidemiology, etc. These meta-analyses not only guide future research but also help policymakers make informed decisions. 

- Methodological guide on conducting meta-analyses of proportions

   My comprehensive tutorial offers clear, structured methodological guidance to applied researchers on how to conduct meta-analyses of proportion in the R programming environment. It presents a step-by-step workflow and a critical review on common practices that could lead to biased effect estimates or misleading conclusions. Over the past five years, this tutorial has garnered significant attention from the international research community, as evidenced by more than 53,000 reads on ResearchGate and 135 citations on Google Scholar:

    * **Wang, N.** (2023). Conducting meta-analyses of proportions in R. _Journal of Behavioral Data Science_, 3(2), 1-63. [Available here](https://www.researchgate.net/publication/375451196_Conducting_Meta-analyses_of_Proportions_in_R).

- Generalized linear mixed models (GLMMs) vs. Freeman-Tukey double arcsine transformation method
  
  Recent studies have criticized the most popular proportional data transformation method for meta-analysis, the Freeman-Tukey double arcsine transformation, due to its potential for producing misleading results in extreme scenarios. Central to this discussion is whether GLMMs offer a more robust alternative for handling cases of rare events or small sample sizes. In recognition of the importance of this ongoing discussion, my future research will involve an extensive simulation study comparing the GLMMs and the double arcsine transformation in meta-analyses of proportions, paying special attention to scenarios where extreme event rates or diverse sample sizes across primary studies occur. This work aims to provide a much-needed methodological guide for applied researchers and potentially elevate the quality and reliability of future meta-analyses in various scientific fields. 
