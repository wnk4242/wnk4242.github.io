---
layout: archive
title: "Research"
permalink: /research/
author_profile: true
---
## Research Replicability
Over the last decade, psychology has faced a replication crisis because efforts to replicate past study findings often fail to yield consistent results. Psychologists have conducted several large-scale multi-lab replication projects (such as the Many Labs series) to reassess the replicability of earlier findings. I am currently pursuing three lines of research to enhance our understanding of research replicability:

- The application of Bayesian meta-analysis in studies of research replicability

   Psychologists are currently faced with a pressing methodological challenge: developing a universally accepted, empirically validated framework to evaluate outcomes of replications. I propose adopting the Bayesian 
   hypothesis testing within a meta-analytic framework and explore how well such an approach answers the following question: _“Do the pooled data from all replications support or refute the presence of the psychological 
   effect?”_ More importantly, my research expands our understanding by answering an essential follow-up question: _“How often are our conclusions about the replicability of original studies incorrect, based on their 
   replications?”_ Moving forward, my aim is to apply the advancements from my current research to empirical replication data. This effort aligns with my long-term objective of developing a statistical framework designed to 
   assess the replications of advanced and complex research designs. 

- Effect size, heterogeneity, and power of exact replications

   Effect-size variability (also known as heterogeneity), is a key methodological factor that is frequently neglected in planning the sample sizes for replication studies. My research investigates several fundamental 
   questions regarding heterogeneity in replication studies and examines how the potentially linear relationship between effect sizes and heterogeneity may impact the statistical power of replications. Gaining a better 
   understanding of heterogeneity is key to improving sample size planning for future multi-lab replication projects. Moving forward, I plan to assess the impact of power analysis methods that account for heterogeneity 
   compared to traditional methods that typically ignore it. My ultimate goal is to develop strategies for determining optimal sample sizes and the appropriate number of replications.

- Credibility of research findings in educational psychology
  
   Statistical power plays a key role in ensuring the reliability, precision, and replicability of effect size estimation. In psychology, the average power is distressingly low—estimated at 36% with only 8% of studies deemed 
   adequately powered. The impact of such issues on educational psychology has not received the attention it deserves. I am currently researching the credibility (true positive rate) of meta-analyses published in five 
   leading educational psychology journals between 2012 and 2022. Completion of this work will raise awareness within the academic community about the prevalence of under-powered research in top-tier journals. 

## Meta-analysis of Proportions

An important application of meta-analytic methods is pooling single proportions to estimate the overall prevalence of a specific condition. In 2017 alone, 152 meta-analyses of proportions were published across various scientific disciplines, such as medicine, public health, epidemiology, etc. These meta-analyses not only guide future research but also help policymakers make informed decisions. 

I wrote a comprehensive manuscript that was initially shared as a [preprint](https://www.researchgate.net/publication/325486099_How_to_Conduct_a_Meta-Analysis_of_Proportions_in_R_A_Comprehensive_Tutorial/citations) on ResearchGate and has since evolved into a peer-reviewed publication. The paper offers clear, structured methodological guidance and specific designs for use with the R programming environment. It presents a step-by-step workflow and a critical review on common practices that could lead to biased effect estimates or misleading conclusions. Over the past five years, this methodological guide has garnered significant attention from the international research community, as evidenced by more than 52,000 reads on ResearchGate and 134 citations on Google Scholar: 
- **Wang, N.** (2023). [Conducting meta-analyses of proportions in R](https://www.researchgate.net/publication/375451196_Conducting_Meta-analyses_of_Proportions_in_R). _Journal of Behavioral Data Science_, 3(2), 1-63.
